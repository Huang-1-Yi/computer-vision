{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c0898b4-6d59-4767-ab49-a887d8adfad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from deepface import DeepFace\n",
    "import faiss\n",
    "import os\n",
    "\n",
    "# Load YuNet model for face detection\n",
    "yunet = cv2.FaceDetectorYN.create(\n",
    "    model=\"face_detection_yunet_2023mar.onnx\",  # Pre-trained ONNX model path\n",
    "    config=\"\",\n",
    "    input_size=(320, 320),  # Input image size\n",
    "    score_threshold=0.9,\n",
    "    nms_threshold=0.3,\n",
    "    top_k=5000\n",
    ")\n",
    "\n",
    "# Load FaceNet model for embeddings\n",
    "facenet_model = \"Facenet\"  # You can also use 'Facenet512' for higher accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b67511c6-9a8b-4a9f-b281-a3f7c3db89d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_crop_faces(image_path, return_boxes=False):\n",
    "    \"\"\"\n",
    "    Detect faces in an image, crop them, and optionally return face bounding boxes.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the input image.\n",
    "        return_boxes (bool): Whether to return face bounding boxes.\n",
    "\n",
    "    Returns:\n",
    "        list: List of cropped face images.\n",
    "        list (optional): List of bounding boxes [(x, y, width, height), ...].\n",
    "    \"\"\"\n",
    "    # Read the input image\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Could not read {image_path}\")\n",
    "        return [] if not return_boxes else ([], [])\n",
    "\n",
    "    # Set YuNet input size\n",
    "    height, width = img.shape[:2]\n",
    "    yunet.setInputSize((width, height))\n",
    "    \n",
    "    # Detect faces\n",
    "    _, faces = yunet.detect(img)\n",
    "\n",
    "    cropped_faces = []\n",
    "    face_boxes = []\n",
    "\n",
    "    if faces is not None:\n",
    "        for face in faces:\n",
    "            x, y, w, h = face[:4].astype(int)\n",
    "            # Crop the face from the image\n",
    "            cropped_face = img[y:y+h, x:x+w]\n",
    "            cropped_faces.append(cropped_face)\n",
    "            face_boxes.append((x, y, w, h))  # Store the bounding box coordinates\n",
    "\n",
    "    if return_boxes:\n",
    "        return cropped_faces, face_boxes\n",
    "    return cropped_faces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "282137d8-d0c1-4a81-aa1c-fd1637ec9cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the pre-trained FaceNet model\n",
    "\n",
    "\n",
    "def get_embeddings(face_images):\n",
    "    embeddings = []\n",
    "    for face_img in face_images:\n",
    "        face_rgb = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "        embedding = DeepFace.represent(img_path=face_rgb, model_name=facenet_model,enforce_detection=False)[0]['embedding']\n",
    "        embeddings.append(np.array(embedding, dtype=np.float32))\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a64829ac-af17-4efb-86e8-299c46c45552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize FAISS index\n",
    "embedding_dimension = 128  # FaceNet output dimension\n",
    "index = faiss.IndexFlatL2(embedding_dimension)  # L2 (Euclidean) distance\n",
    "photo_ids = []  # Keep track of photo IDs to map results\n",
    "\n",
    "def process_and_store_images(image_folder):\n",
    "    for photo_id, image_file in enumerate(os.listdir(image_folder)):\n",
    "        image_path = os.path.join(image_folder, image_file)\n",
    "        faces = detect_and_crop_faces(image_path)\n",
    "        if faces:\n",
    "            embeddings = get_embeddings(faces)\n",
    "            for embedding in embeddings:\n",
    "                index.add(np.expand_dims(embedding, axis=0))  # Add embedding to FAISS\n",
    "                photo_ids.append(image_file)\n",
    "    print(f\"Stored {len(photo_ids)} faces in FAISS index.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d6db8b6-eee5-4400-9906-870cd58e85d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_faces(query_image, top_k=5):\n",
    "    faces = detect_and_crop_faces(query_image)\n",
    "    if not faces:\n",
    "        print(\"No face detected in query image.\")\n",
    "        return\n",
    "\n",
    "    query_embeddings = get_embeddings(faces)\n",
    "\n",
    "    for query_embedding in query_embeddings:\n",
    "        distances, indices = index.search(np.expand_dims(query_embedding, axis=0), top_k)\n",
    "        print(\"Top Matches:\")\n",
    "        for dist, idx in zip(distances[0], indices[0]):\n",
    "            if idx >= 0:\n",
    "                print(f\"Image: {photo_ids[idx]}, Distance: {dist}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f32dfb3e-719c-461d-a049-364219d10fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 64 faces in FAISS index.\n"
     ]
    }
   ],
   "source": [
    "image_folder = \"drive\"  # Folder with all images\n",
    "process_and_store_images(image_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86f5310b-e234-474b-8c0f-1e26a8ad3d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Matches:\n",
      "Image: 1F2A5973.JPG, Distance: 12.011140823364258\n",
      "Image: _SAN4237.JPG, Distance: 56.65167999267578\n",
      "Image: _SAN4171.JPG, Distance: 92.5196762084961\n",
      "Image: 1F2A5501.JPG, Distance: 103.70606231689453\n",
      "Image: _SAN4171.JPG, Distance: 105.64873504638672\n"
     ]
    }
   ],
   "source": [
    "query_image = \"img_3.png\"\n",
    "search_similar_faces(query_image, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cdd61152-ff67-4fbc-98cf-1a80e0a975e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index, \"faiss.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17926d35-db55-48ee-b17c-2864911a296f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
