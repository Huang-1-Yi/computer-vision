{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37ebdf39-b94d-4f54-adfd-879a5746e678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# script to generate embeddings and perform similarity searches\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoImageProcessor, EfficientNetModel, ViTModel, AutoModel, CLIPProcessor, CLIPModel, Blip2Processor, Blip2Model\n",
    "from torchvision import models, transforms\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4039e59f-6a76-4a5a-b450-4942006798a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "flags_df = pd.read_csv('national_flags.csv')  # Uncomment if you're loading from a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9eb5d42-be1f-496d-9940-638249bd1886",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIR = \"images\"\n",
    "def load_local_image(country_name):\n",
    "    # Sanitize the country name to match the local image file naming convention\n",
    "    sanitized_country_name = country_name.replace(\" \", \"_\").replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "    \n",
    "    # Path to the local image file\n",
    "    image_path = os.path.join(IMAGE_DIR, f\"{sanitized_country_name}.png\")\n",
    "\n",
    "    # Check if the image exists in the folder\n",
    "    if os.path.exists(image_path):\n",
    "        img = Image.open(image_path)\n",
    "        \n",
    "        # Convert image to RGB if not already in that mode\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "        \n",
    "        return img\n",
    "    else:\n",
    "        print(f\"Image for {country_name} not found.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a8166e5d-a1be-482a-9f64-b5fd2c2ccb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ViT\n",
    "\n",
    "def extract_features_vit(country):\n",
    "    image_processor = AutoImageProcessor.from_pretrained(\"google/vit-large-patch16-224-in21k\")\n",
    "    model = ViTModel.from_pretrained(\"google/vit-large-patch16-224-in21k\")\n",
    "    \n",
    "    # prepare input image\n",
    "    img = load_local_image(country)\n",
    "    inputs = image_processor(img, return_tensors='pt')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embedding = outputs.last_hidden_state\n",
    "    embedding = embedding[:, 0, :].squeeze(1)\n",
    "    return embedding.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e9be4b1f-5af1-41a4-9c5d-73b908e65318",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EfficientNet\n",
    "\n",
    "def extract_features_efficientNet(country):\n",
    "    # load pre-trained image processor for efficientnet-b7 and model weight\n",
    "    image_processor = AutoImageProcessor.from_pretrained(\"google/efficientnet-b7\")\n",
    "    model = EfficientNetModel.from_pretrained(\"google/efficientnet-b7\")\n",
    " \n",
    "    # prepare input image\n",
    "    img = load_local_image(country)\n",
    "    inputs = image_processor(img, return_tensors='pt')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "    \n",
    "    embedding = outputs.hidden_states[-1]\n",
    "    embedding = torch.mean(embedding, dim=[2,3])\n",
    "    return embedding.numpy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5fa2cedb-3901-4dca-9ddf-42195e50f90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DINO-v2\n",
    "\n",
    "def extract_features_DINO_v2(country):\n",
    "    # load pre-trained image processor for efficientnet-b7 and model weight\n",
    "    image_processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n",
    "    model = AutoModel.from_pretrained('facebook/dinov2-base')\n",
    "    \n",
    "    # prepare input image\n",
    "    img = load_local_image(country)\n",
    "    inputs = image_processor(img, return_tensors='pt')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embedding = outputs.last_hidden_state\n",
    "    embedding = embedding[:, 0, :].squeeze(1)\n",
    "    return embedding.numpy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3c37b234-23cf-4a4b-8f0b-f11ab4331219",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLIP\n",
    "\n",
    "def extract_features_clip(country):\n",
    "    # load pre-trained image processor for efficientnet-b7 and model weight\n",
    "    image_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    # prepare input image\n",
    "    img = load_local_image(country)\n",
    "    inputs = image_processor(images=img, return_tensors='pt', padding=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embedding = model.get_image_features(**inputs) \n",
    "    return embedding.numpy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "993336ac-3950-4269-a38b-fc76dd024c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_blip(country):\n",
    "    image_processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "    model = Blip2Model.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16)\n",
    "    img = load_local_image(country)\n",
    "    inputs = image_processor(images=img, return_tensors='pt', padding=True)\n",
    "    print('input shape: ', inputs['pixel_values'].shape)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.get_qformer_features(**inputs)\n",
    "    embedding = outputs.last_hidden_state\n",
    "    embedding = embedding[:, 0, :].squeeze(1)\n",
    "    return embedding.numpy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7bbade27-cc88-4b77-8330-cfa1aad219e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_vgg16(country):\n",
    "    model = models.vgg16(pretrained=True) \n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # Define the transformation to preprocess the image\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    img = load_local_image(country)\n",
    "    img_t = preprocess(img)\n",
    "    batch_t = torch.unsqueeze(img_t, 0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embedding = model(batch_t)\n",
    "    return embedding.numpy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef91ece-b4c9-4214-af1c-3c9a3618a2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for all flags\n",
    "flags_df['features'] = flags_df['Country'].apply(extract_features_vit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "68c5510d-19bb-47d1-88da-92a15e6c93e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export embeddings to CSV\n",
    "flags_df.to_csv('national_flag_embeddings_blip.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d35fed-3be5-40d7-9ea8-4817ac88beda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cosine similarity with FAISS\n",
    "\n",
    "df = pd.read_csv('embeddings/national_flag_embeddings_vit.csv')\n",
    "country = \"Australia\"\n",
    "\n",
    "def clean_feature_string(feature_str):\n",
    "    cleaned_str = re.sub(r'[\\[\\]]', '', feature_str)  # Remove brackets\n",
    "    cleaned_values = np.fromstring(cleaned_str, sep=' ')  # Parse values into numpy array\n",
    "    return cleaned_values\n",
    "\n",
    "# Function to get top K similar countries using FAISS\n",
    "def get_top_k_similar_countries(input_country, k=5):\n",
    "    print(df)\n",
    "    countries = df['Country'].values\n",
    "    features = np.array([clean_feature_string(f) for f in df['features'].values])\n",
    "    \n",
    "    # Find the index of the input country\n",
    "    try:\n",
    "        input_idx = list(countries).index(input_country)\n",
    "    except ValueError:\n",
    "        return f\"Country '{input_country}' not found in the dataset.\"\n",
    "    \n",
    "    input_embedding = features[input_idx].reshape(1, -1)\n",
    "\n",
    "    # Create a FAISS index for similarity search\n",
    "    dim = features.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)  # Use L2 distance (can be changed to IndexFlatIP for cosine similarity)\n",
    "    \n",
    "    # Add all features to the FAISS index\n",
    "    index.add(features)\n",
    "    \n",
    "    # Search for the top K most similar countries\n",
    "    distances, top_k_idx = index.search(input_embedding, k+1)  # k+1 to exclude the country itself\n",
    "    \n",
    "    # Return top K countries with their similarity scores\n",
    "    return [(countries[i], distances[0][j]) for j, i in enumerate(top_k_idx[0]) if i != input_idx]\n",
    "\n",
    "# Display top 5 similar flags \n",
    "top_5_countries = get_top_k_similar_countries(country, k=5)\n",
    "\n",
    "for idx, (country, score) in enumerate(top_5_countries):\n",
    "    # Load the flag image for each country from the local folder\n",
    "    img = load_local_image(country)\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9fd287-3972-40cd-b2f8-063f16719c53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
